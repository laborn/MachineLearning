---
title: "Machine Learning Project"
author: "LuAnn Born"
date: "Friday, December 19, 2014"
output: html_document
---
**Overview**

This project will explore data obtained from six subjects that performed barbell lifts. The data used included 160 variables including measurements from accelerometers on the subjects' belt, forearm, arm and dumbbell.

**Model Development**

After examining the training and testing sets it was determined that only 52 variables were actually applicable, since the testing set did not include rolled up statistical data such as means, variances, standard deviations, etc. that were present in the training set. Other excluded variables were the timestamps and min and max variables.

After narrowing down the possible variables that might give a good model fit, a seed was used to produce reproducible results and the model was trained with the training data using the Random Forest algorithm, the classe variable as the outcome and the 52 variables that actually had data in the testing set for predictors.

Once this completed the variable importance (varImp) function was run to determine which variables might be most informative. Predictions were made with the caret function predict on the model using the pml.testing data set. I had a 100% success rate in the submission phase of this project.

The first model took about an hour to complete, so I retrained with the top 20 important variables to see if I could get better performance and maintain the accuracy. The second model took roughly 20 minutes and came up with the same results for the testing set. I eliminated another ten and the time to model that took about 12 minutes with the same results. The last model was to eliminate the bottom ranked five and rerun to look at those results. The result was a final time of 7 minutes and the same results.
```{r}
PkgNames <- c("ElemStatLearn", "caret", "randomForest", "stats")
invisible(suppressMessages(suppressWarnings(lapply(PkgNames, require, character.only = T))))
pml.training <- read.csv("pml-training.csv")
pml.testing <- read.csv("pml-testing.csv")
inTrain <- createDataPartition(y=pml.training$classe, p=0.7, list=FALSE)
training <- pml.training[inTrain,]
testing <- pml.training[-inTrain,]
set.seed(10901)
Sys.time()
treeMod1 = train(classe ~roll_belt+pitch_belt+yaw_belt+total_accel_belt+
                   gyros_belt_x+gyros_belt_y+gyros_belt_z+accel_belt_x+
                   accel_belt_y+accel_belt_z+magnet_belt_x+magnet_belt_y+
                   magnet_belt_z+roll_arm+pitch_arm+yaw_arm+total_accel_arm+
                   gyros_arm_x+gyros_arm_y+gyros_arm_z+accel_arm_x+
                   accel_arm_y+accel_arm_z+magnet_arm_x+magnet_arm_y+
                   magnet_arm_z+roll_dumbbell+pitch_dumbbell+yaw_dumbbell+
                   total_accel_dumbbell+gyros_dumbbell_x+gyros_dumbbell_y+
                   gyros_dumbbell_z+accel_dumbbell_x+accel_dumbbell_y+
                   accel_dumbbell_z+magnet_dumbbell_x+magnet_dumbbell_y+
                   magnet_dumbbell_z+roll_forearm+pitch_forearm+yaw_forearm+
                   total_accel_forearm+gyros_forearm_x+gyros_forearm_y+
                   gyros_forearm_z+accel_forearm_x+accel_forearm_y+
                   accel_forearm_z+magnet_forearm_x+magnet_forearm_y+magnet_forearm_z,
                   method="rf", data=training)
Sys.time()
varImp(treeMod1)
print(treeMod1$finalModel)
head(treeMod1$resample)
confusionMatrix(testing$classe, predict(treeMod1,testing))
predict(treeMod1, pml.testing)
Sys.time()
treeMod2 = train(classe ~roll_belt+pitch_belt+yaw_belt+gyros_belt_z+accel_belt_z+
                   magnet_belt_y+magnet_belt_z+roll_arm+roll_dumbbell+yaw_dumbbell+
                   total_accel_dumbbell+accel_dumbbell_x+accel_dumbbell_y+
                   accel_dumbbell_z+magnet_dumbbell_x+magnet_dumbbell_y+
                   magnet_dumbbell_z+roll_forearm+pitch_forearm+accel_forearm_x, method="rf",
                   data=training)
Sys.time()
varImp(treeMod2)
plot(varImp(treeMod2))
print(treeMod2$finalModel)
head(treeMod2$resample)
confusionMatrix(testing$classe, predict(treeMod2,testing))
predict(treeMod2, pml.testing)
Sys.time()
treeMod3 = train(classe ~roll_belt+yaw_belt+magnet_dumbbell_z+magnet_dumbbell_y+pitch_belt+
                   pitch_forearm+magnet_dumbbell_x+roll_forearm+accel_belt_z+accel_dumbbell_y,
                   method="rf", data=training)
Sys.time()
varImp(treeMod3)
plot(varImp(treeMod3))
print(treeMod3$finalModel)
head(treeMod3$resample)
confusionMatrix(testing$classe, predict(treeMod3,testing))
predict(treeMod3, pml.testing)
Sys.time()
treeMod = train(classe ~roll_belt+yaw_belt+pitch_belt+pitch_forearm+magnet_dumbbell_z,
                  method="rf", data=training)
Sys.time()
varImp(treeMod)
plot(varImp(treeMod))
print(treeMod$finalModel)
head(treeMod$resample)
confusionMatrix(testing$classe, predict(treeMod,testing))
predict(treeMod, pml.testing)
```
As can be seen from the above, random forest does not necessarily require cross validations. The documentation states that it is performed internally as part of the function. Regardless, I did use the training set to cross validate by splitting that into 
training and testing data sets. The original model worked effectively and gave 100% accuracy in the pml.testing data set.
The out of bag expected error rates, accuracies, and times to compile the model are given below for the models using the cross
validated training and testing data sets for a previous run and are as follows:

**Model     |    Expected Error Rate  |   Accuracy  |  Time to Compile**
------------|-------------------------|-------------|-------------------

treeMod1    |        0.79%            |   99.51%    |      54 mins.

treeMod2    |        1.16%            |   99.01%    |      20 mins.

treeMod3    |        1.38%            |   98.88%    |      12 mins.

treeMod     |        2.98%            |   96.89%    |       7 mins.

The biggest variable found in model selection was the time factor and so for the sake of speed in the case of small data sets the treeMod model would be the best choice. Accuracy would prove to be more of a problem once there the data size reaches into the
hundreds of records. In that case one of the other models could be chosen to guarantee a higher accuracy rate.
